{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize SuperLearner results\n",
    "\n",
    "Below are some examples for how to load and plot results from the SuperLearner.\n",
    "\n",
    "The key inputs to this notebook are specified in the\n",
    "1. `model_dir` variable in **Load SuperLearner models**\n",
    "2. `num_inputs`, `data_df`, and `target_name` in **Use the SuperLearner models** and\n",
    "3. `permute_str` and `job_list` in **Feature permutation importance**\n",
    "\n",
    "## Dependencies\n",
    "Before you can run this notebook, you need to install the packages listed in `sl_requirements.txt` into your Python environment.  Using Miniconda is recommended; once Miniconda is installed, the following command on the terminal should install the required packages in your environment:\n",
    "```bash\n",
    "while read requirement; do conda install --yes -c conda-forge $requirement; done < sl_requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SuperLearner models\n",
    "\n",
    "The SuperLearner is a blend of several stacked models.  They are all stored in the same file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the location of the files that store the SuperLearner\n",
    "# superlearner_conf.py and SuperLearners.pkl are BOTH needed.\n",
    "\n",
    "#======================\n",
    "# Updated Model for fifth MODEX iteration (4th loop turn, Oct. 2022)\n",
    "#======================\n",
    "jobid = 60357\n",
    "num_inputs=25\n",
    "\n",
    "#===========================================================\n",
    "# Load data for use in training and evaluating SuperLearner\n",
    "#===========================================================\n",
    "data_dir = str(jobid)+'/model_dir'\n",
    "train_df = pd.read_csv(data_dir+'/train.csv').astype(np.float32)\n",
    "X_train = train_df.values[:, :num_inputs]\n",
    "Y_train = train_df.values[:, num_inputs:]\n",
    "\n",
    "test_df = pd.read_csv(data_dir+'/test.csv').astype(np.float32)\n",
    "X_test = test_df.values[:, :num_inputs]\n",
    "Y_test = test_df.values[:, num_inputs:]\n",
    "\n",
    "all_df = pd.concat((train_df,test_df),axis=0)\n",
    "    \n",
    "X_all = all_df.values[:, :num_inputs]\n",
    "Y_all = all_df.values[:, num_inputs:]\n",
    "\n",
    "# The following code should do the same thing but keeping\n",
    "# the data as a dataframe, not just an array.\n",
    "# Set the main output variable\n",
    "target_name = 'rate.mg.per.L.per.h'\n",
    "# Pull the target column out, and remove it from data_df.\n",
    "target_train_df = train_df.pop(target_name)\n",
    "target_test_df = test_df.pop(target_name)\n",
    "target_all_df = all_df.pop(target_name)\n",
    "\n",
    "#===========================================================\n",
    "# Load the SuperLearner models\n",
    "#===========================================================\n",
    "model_dir = str(jobid)+'/model_dir/'\n",
    "sys.path.append(model_dir)\n",
    "with open(model_dir+'SuperLearners.pkl','rb') as file_object:\n",
    "    superlearner = pickle.load(file_object)\n",
    "    \n",
    "#===========================================================\n",
    "# Uncomment this line to see a dictionary that organizes the models\n",
    "# by output variable (each entry in dictionary is an output).\n",
    "#superlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given output variable, list the attributes:\n",
    "superlearner['rate.mg.per.L.per.h'].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given output variable (in this case there is only one) list the underlying models:\n",
    "list_models = list(superlearner['rate.mg.per.L.per.h'].named_estimators_.keys())\n",
    "list_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An individual model can be accessed in this way:\n",
    "#superlearner['rate.mg.per.L.per.h'].named_estimators_['mlp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the parts of the model are more succinctly listed here:\n",
    "#superlearner['rate.mg.per.L.per.h'].named_estimators_['nusvr-rbf'].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the list of weights of the models can be accessed with:\n",
    "# (Note the _ are important and have to do with scikit learn\n",
    "# naming conventions of variables that are set after fitting.)\n",
    "sl_weights = superlearner['rate.mg.per.L.per.h'].final_estimator_.weights_\n",
    "sl_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the weights to get a list of the models \n",
    "# that have been included in the SuperLearner\n",
    "sl_models = []\n",
    "mm = 0\n",
    "for model in list_models:\n",
    "    #print(model)\n",
    "    sl_weights[mm]\n",
    "    if ( sl_weights[mm] > 0.1 ):\n",
    "        sl_models.append(model)\n",
    "    mm = mm + 1\n",
    "sl_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the SuperLearner models\n",
    "\n",
    "The models can be used either all together (i.e. \"stacked\") or individually.  The stacked ensemble is weighted by how well each model helped fit the training data.  (An example of the weights is in the cell immendiately above with `superlearner[].final_estimator_.weights_`.)  Let's do a very simple demo of how well the stacked ensemble makes predictions compared to the predictions of each individual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and metric with the stacked regressor\n",
    "Y_hat_train = superlearner['rate.mg.per.L.per.h'].predict(X_train)\n",
    "Y_hat_test = superlearner['rate.mg.per.L.per.h'].predict(X_test)\n",
    "\n",
    "# Numpy's corrcoef is the \"classical\" correlation coefficient\n",
    "# whose square represents the percent of the variability of Y\n",
    "# \"explained\" by Y_hat assuming a linear relationship. [0,1]\n",
    "print('Classical: Pearson correlation squared, r2 = '+str(np.min(np.corrcoef(np.squeeze(Y_train),Y_hat_train)**2)))\n",
    "print('Hold out: Pearson correlation squared, r2 = '+str(np.min(np.corrcoef(np.squeeze(Y_test),Y_hat_test)**2)))\n",
    "print('-----------------------------------------------------')\n",
    "# The coefficient of variation is a more general measure\n",
    "# that does not assume an underlying linear relationship.\n",
    "# This value can be negative if the model's predictions \n",
    "# are worse than just guessing the average all the time\n",
    "# (i.e. the \"persistance forecast\" as a baseline).\n",
    "print('Classical: Coefficient of variation, R2 = '+str(superlearner['rate.mg.per.L.per.h'].score(X_train,Y_train)))\n",
    "print('Hold out: Coefficient of variation, R2 = '+str(superlearner['rate.mg.per.L.per.h'].score(X_test,Y_test)))\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "# Compute line of best fit between testing and training targets\n",
    "test_line = np.polynomial.polynomial.Polynomial.fit(\n",
    "    np.squeeze(Y_test),\n",
    "    np.squeeze(Y_hat_test),1)\n",
    "print(test_line)\n",
    "test_xy = test_line.linspace(n=100,domain=[Y_all.min(),Y_all.max()])\n",
    "\n",
    "train_line = np.polynomial.polynomial.Polynomial.fit(\n",
    "    np.squeeze(Y_train),\n",
    "    np.squeeze(Y_hat_train),1)\n",
    "print(train_line)\n",
    "train_xy = train_line.linspace(n=100,domain=[Y_all.min(),Y_all.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics associated with the line of\n",
    "# best fit between Y_test and Y_hat_test.  The idea\n",
    "# of this line is to quantify to what extent Y_test\n",
    "# can be used to predict Y_hat_test -> for a given\n",
    "# known target, what is returned by the ML model\n",
    "# and what is the uncertainty in that value?\n",
    "#\n",
    "# EXPECTATION: Given the bias in the ML model\n",
    "# towards the mean values of training data, \n",
    "# we would expect greater uncertainty with\n",
    "# greater values of Y_test and Y_hat_test.\n",
    "\n",
    "# UPDATE: Actually, our goal is to predict to\n",
    "# what extent we can use Y_hat_test to predict\n",
    "# the error since we'll be using Y_hat to estimate\n",
    "# the errors.  In this case, this is exactly what\n",
    "# the error in the predictons, s_p is getting at.\n",
    "# Note that there was an error below in the SSE\n",
    "# on Oct 29, 2021, sse_error should have been in the\n",
    "# calculation, but since sse_error ~ sse, the results\n",
    "# are not appreciably different.\n",
    "\n",
    "# Equations based on McClave & Dietrich, \n",
    "# _Statistics_, 6th Ed., pp. 672, 682, 707.\n",
    "\n",
    "n_sample_size = np.size(Y_test)\n",
    "print('N = '+str(n_sample_size))\n",
    "\n",
    "x_bar = np.mean(Y_test)\n",
    "y_bar = np.mean(Y_hat_test)\n",
    "\n",
    "ssxx = np.sum((np.squeeze(Y_test)-np.mean(Y_test))**2)\n",
    "print('SSxx = '+str(ssxx))\n",
    "\n",
    "ssyy = np.sum((np.squeeze(Y_hat_test)-np.mean(Y_hat_test))**2)\n",
    "print('SSyy = '+str(ssyy))\n",
    "\n",
    "ssxy = np.sum((np.squeeze(Y_hat_test)-np.mean(Y_hat_test))*(np.squeeze(Y_test)-np.mean(Y_test)))\n",
    "print('SSxy = '+str(ssxy))\n",
    "\n",
    "slope = ssxy/ssxx               \n",
    "print('slope = '+str(slope))\n",
    "\n",
    "intercept = y_bar - x_bar*slope\n",
    "print('intercept = '+str(intercept))\n",
    "\n",
    "# Sum squared of the errors in the testing predictions\n",
    "sse = np.sum((np.squeeze(Y_test)-Y_hat_test)**2)\n",
    "print('SSE = '+str(sse))\n",
    "\n",
    "# When trying to predict error based on Y_hat,\n",
    "# the sse_error = sum((error_i - mean(error))^2)\n",
    "# which when expanded algebreically\n",
    "sse_error = ssxx + ssyy - 2.0*ssxy\n",
    "print('SSE_error = '+str(sse_error))\n",
    "\n",
    "# Estimator for the standard error of regression between\n",
    "# true targets and predicted targets.\n",
    "sse = sse_error # Do this if using the test predictions to estimate the error\n",
    "s = np.sqrt(sse/(n_sample_size-2))\n",
    "print('s = '+str(s))\n",
    "\n",
    "# Estimate of the sampling distribution of the predictions\n",
    "# of the mean value of the targets at specific value.\n",
    "# This is nice, but it's very flat -> does not change much\n",
    "# based on Y_test.\n",
    "ssxx = ssyy # (Include this line to for Y_hat_test as a predictor of error, otherwise Y_test is the predictor.)\n",
    "s_y = 2*s*np.sqrt((1/n_sample_size) + ((np.squeeze(Y_test)-np.mean(Y_test))**2)/ssxx)\n",
    "s_p = 2*s*np.sqrt(1+(1/n_sample_size) + ((np.squeeze(Y_test)-np.mean(Y_test))**2)/ssxx)\n",
    "\n",
    "#====================================\n",
    "# Empirical error estimate\n",
    "#====================================\n",
    "# Root squared errors\n",
    "rse = np.sqrt((np.squeeze(Y_test)-Y_hat_test)**2)\n",
    "\n",
    "# Supposedly the \"best\" way\n",
    "error_line = np.polynomial.polynomial.Polynomial.fit(\n",
    "    np.squeeze(Y_test),\n",
    "    rse,1)\n",
    "error_line_xy = error_line.linspace(n=100,domain=[Y_test.min(),Y_test.max()])\n",
    "\n",
    "# I prefer:\n",
    "rse_slope = np.sum((rse-np.mean(rse))*(np.squeeze(Y_test)-np.mean(Y_test)))/ssxx\n",
    "rse_inter = np.mean(rse) - rse_slope*np.mean(Y_test)\n",
    "\n",
    "#=========================\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y_test,rse,'ko')\n",
    "#ax.plot(Y_hat_test,rse,'k+')\n",
    "#ax.plot(Y_train,np.sqrt((np.squeeze(Y_train)-Y_hat_train)**2),'k.')\n",
    "ax.plot(Y_test,s_y,'ro')\n",
    "ax.plot(Y_test,s_p,'co')\n",
    "ax.plot(Y_test,s*np.ones(np.shape(Y_test)),'r.')\n",
    "#ax.plot(error_line_xy[0],error_line_xy[1],'g-')\n",
    "#ax.plot(Y_train,rse_inter+rse_slope*Y_train,'go')\n",
    "ax.set_ylabel('Error metric [mg O2/L/h]')\n",
    "ax.set_xlabel('Target respiration rate [mg O2/L/h]')\n",
    "ax.legend(['RSE','Error in mean','Prediction Error','Regression Error'],loc='lower left')\n",
    "ax.grid()\n",
    "#ax.set_ylim([0,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot stacked model, predict with each submodel, plot each submodel\n",
    "#plt.rcParams.update({'font.size': 22})\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(Y_train,np.squeeze(Y_hat_train),'ko',markersize=10)\n",
    "ax.plot(Y_test,np.squeeze(Y_hat_test),'ko',markersize=10,fillstyle='none')\n",
    "ax.plot(test_xy[0],test_xy[1],'r-')\n",
    "ax.plot(train_xy[0],train_xy[1],'r--')\n",
    "ax.grid()\n",
    "ax.set_xlabel('Original target values, mg O2/L/h')\n",
    "ax.set_ylabel('Model predictions, mg O2/L/h')\n",
    "\n",
    "# Predict and metric with the individual models\n",
    "# (The same thing can be achieved with built in:\n",
    "# superlearner['rate.mg.per.L.per.h'].transform(X)\n",
    "# but done explicitly here.)\n",
    "for model_name in list_models:\n",
    "#for model_name in sl_models: # For a slightly simpler plot\n",
    "    print('For model '+model_name+' ----------')\n",
    "    model_object = superlearner['rate.mg.per.L.per.h'].named_estimators_[model_name]\n",
    "    Y_hat_train_mod = model_object.predict(X_train)\n",
    "    Y_hat_test_mod = model_object.predict(X_test)\n",
    "    print('---> Classical')\n",
    "    print('---> r2 = '+str(np.min(np.corrcoef(np.squeeze(Y_train),Y_hat_train_mod)**2)))\n",
    "    print('---> R2 = '+str(model_object.score(X_train,Y_train)))\n",
    "    print('---> Hold out')\n",
    "    print('---> r2 = '+str(np.min(np.corrcoef(np.squeeze(Y_test),Y_hat_test_mod)**2)))\n",
    "    print('---> R2 = '+str(model_object.score(X_test,Y_test)))\n",
    "    # Color coded dots\n",
    "    ax.plot(np.concatenate((Y_train,Y_test),axis=0),\n",
    "            np.concatenate((np.squeeze(Y_hat_train_mod),np.squeeze(Y_hat_test_mod)),axis=0),\n",
    "            '.',markersize=5)\n",
    "    # Uniform black dots\n",
    "    #ax.plot(Y,np.squeeze(Y_hat_mod),'k.',markersize=5)\n",
    "    #ax.plot(Y_test,np.squeeze(Y_hat_test_mod),'k.',markersize=5)\n",
    "    \n",
    "# One-to-one line\n",
    "ax.plot(Y_all,Y_all,'k')\n",
    "\n",
    "# Plot stacked values one more time (for DOE figure)\n",
    "#ax.plot(Y,np.squeeze(Y_hat),'ro',markersize=10)\n",
    "#ax.plot(Y_test,np.squeeze(Y_hat_test),'ro',markersize=10)\n",
    "\n",
    "# Set zoom\n",
    "ax.set_xlim([-45,0])\n",
    "ax.set_ylim([-45,0])\n",
    "\n",
    "# Legend\n",
    "ax.legend(['Stacked TRAIN','Stacked TEST','TEST corr','TRAIN corr']+list_models+['one-to-one'])\n",
    "\n",
    "#ax.errorbar(Y_test,intercept+slope*Y_test,yerr=s_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biases in models due to distribution of target values\n",
    "The SL seems to be biased with lower respiration rates (values closer to zero).  This may be due to the fact that the ML models are fit on a dataset that has lots of values in the range 0 to -10 and fewer respiration rates that are greater.  The histogram below quantifies this distribution and it seems that -15 mg O2/L/h is a cut point for perhaps training two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "n, bins, patches = ax.hist(Y_train, 20, density=False, facecolor='g', alpha=0.5, align='mid', histtype='stepfilled')\n",
    "n, bins, patches = ax.hist(Y_test, 20, density=False, facecolor='k', alpha=0.5, align='mid', histtype='stepfilled')\n",
    "ax.legend(['Training set','Testing set'])\n",
    "ax.set_xlabel('Respiration rate (target) [mg O2/L/h]')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions with different inputs\n",
    "\n",
    "Now use data from the merged GLORICH/Hydrosheds data set to compute respiration rate on a global scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = True\n",
    "if predict :\n",
    "    # For CONUS results presented to community meeting:\n",
    "    #predict_file_name = 'WH_RA_GL_global_predict_25_inputs.csv'\n",
    "    #predict_file_ixy = 'whondrml_global_train_xy.csv'\n",
    "    #predict_out_file = 'GLORICH_global_predictions_v2.csv'\n",
    "    \n",
    "    # For other results:\n",
    "    #predict_file_name = 'collab_predict_25_inputs.csv'\n",
    "    #predict_file_ixy = 'collab_predict_ixy.csv'\n",
    "    #predict_out_file = 'collab_predict_output.csv'\n",
    "    \n",
    "    predict_file_name = 'WH_RA_GL_global_predict_25_inputs.csv'\n",
    "    predict_file_ixy = 'WH_RA_GL_global_predict_ixy.csv'\n",
    "    predict_out_file = 'GLORICH_global_predictions_updated.csv'\n",
    "    \n",
    "    predict_df = pd.read_csv(\n",
    "        predict_file_name).astype(np.float32)\n",
    "    predict_df.fillna(predict_df.mean(),inplace=True)\n",
    "    X = predict_df.values\n",
    "\n",
    "    # When computing prediction, note that abs() is called to make all latitudes positive\n",
    "    # (i.e. reflected across the equator).  The other variables should all be positive\n",
    "    # so a global abs shouldn't be an issue.\n",
    "    #Y_predict = superlearner['rate.mg.per.L.per.h'].predict(np.abs(X))\n",
    "    # Remove abs operator above since latitude is no longer being used as an input\n",
    "    # (and hasn't for a a very long time)\n",
    "    Y_predict = superlearner['rate.mg.per.L.per.h'].predict(X)\n",
    "\n",
    "    # Estimate the error based on the training data\n",
    "    Y_hat_error = 2*s*np.sqrt((1/n_sample_size) + ((np.squeeze(Y_predict)-np.mean(Y_test))**2)/ssxx)\n",
    "    Y_hat_pred_error = 2*s*np.sqrt(1+(1/n_sample_size) + ((np.squeeze(Y_predict)-np.mean(Y_test))**2)/ssxx)\n",
    "    Y_hat_empir_err = rse_inter+rse_slope*Y_predict\n",
    "    \n",
    "    # Put the predictions with lon lat data separated beforehand.\n",
    "    output_df = pd.read_csv(predict_file_ixy)\n",
    "        \n",
    "    output_df[target_name] = pd.Series(Y_predict)\n",
    "    output_df['mean.error'] = pd.Series(Y_hat_error)\n",
    "    output_df['predict.error'] = pd.Series(Y_hat_pred_error)\n",
    "    #output_df['empirical.error'] = pd.Series(Y_hat_empir_err)\n",
    "    output_df.to_csv(\n",
    "        predict_out_file,\n",
    "        index=False,\n",
    "        na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature permutation importance (FPI)\n",
    "\n",
    "Which of the many possible inputs (i.e. \"features\") is most important?  Here, we address this question by permuting the inputs to see which have the biggest impact on the results.  We also need to experiment with grouping together correlated features.  If features are significantly correlated with each other, then they need to be permuted together, otherwise when a feature is permuted, the model can still get information from the other, correlated features, meaning that the final predicted values may not change substantially -> the importance will be \"diluted\" across the correlated features.\n",
    "\n",
    "Although no model fitting is required, this can TAKE A VERY LONG TIME to run even for a small data set - bigger data sets and exploring variability among models warrents a separate parallelized workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPI functions\n",
    "\n",
    "#=====================================================================================\n",
    "def permute_importance(permutation_feature_blocks_str, model, X, y, scoring_func, n_repeats=20, ratio_score=True):\n",
    "    base_preds = model.predict(X.values)\n",
    "    base_score = scoring_func(y, base_preds)\n",
    "\n",
    "    blocks, block_names = parse_permutation_feature_blocks(permutation_feature_blocks_str, X.columns)\n",
    "    block_scores = list()\n",
    "    for block, block_name in zip(blocks, block_names):\n",
    "        block_df = X.copy()\n",
    "        repeat_scores = list()\n",
    "        for i_repeat in range(n_repeats):\n",
    "            print('For block '+block[0]+' iteration '+str(i_repeat))\n",
    "            block_df[block] = shuffle(block_df[block]).values\n",
    "            repeat_preds = model.predict(block_df.values)\n",
    "            repeat_scores.append(scoring_func(y, repeat_preds))\n",
    "\n",
    "        # Get block score\n",
    "        importance_score_mean = np.mean(repeat_scores) / base_score if ratio_score else np.mean(repeat_scores) - base_score\n",
    "        importance_score_std = np.std(repeat_scores) / base_score if ratio_score else np.std(repeat_scores) - base_score\n",
    "        block_scores.append((block_name, importance_score_mean, importance_score_std))\n",
    "\n",
    "    # Return output sorted by the mean ratio of change (second column)\n",
    "    # For coalescing FPI output from many runs, we don't want this\n",
    "    # sorting - just return the scores unsorted and we'll take the mean\n",
    "    # over many models and then sort later.\n",
    "    #return sorted(block_scores, key=lambda r: r[1], reverse=True)\n",
    "    return block_scores\n",
    "\n",
    "#=====================================================================================\n",
    "def parse_permutation_feature_blocks(permutation_feature_blocks_str, df_column_index):\n",
    "    blocks = [\n",
    "        [bl_item.strip() for bl_item in bl.strip().split(',')]\n",
    "        for bl in permutation_feature_blocks_str.strip().split(';')\n",
    "    ]  if permutation_feature_blocks_str else list()\n",
    "\n",
    "    column_idx = {v: k for k, v in enumerate(df_column_index)}\n",
    "    blocks_ = list()\n",
    "    blocks_names = list()\n",
    "    explicit_blocks = set()\n",
    "    for bl in blocks:\n",
    "        parsed_block = list()\n",
    "        for bl_item in bl:\n",
    "            if ':' in bl_item:\n",
    "                start_col, end_col = bl_item.split(':')\n",
    "                parsed_block.extend(list(df_column_index[column_idx[start_col]:column_idx[end_col] + 1]))\n",
    "            else:\n",
    "                parsed_block.append(bl_item)\n",
    "        blocks_.append(parsed_block)\n",
    "        blocks_names.append(','.join(bl))\n",
    "        explicit_blocks = explicit_blocks.union(set(parsed_block))\n",
    "\n",
    "    for singleton in set(df_column_index) - explicit_blocks:\n",
    "        blocks_.append([singleton])\n",
    "        blocks_names.append(singleton)\n",
    "    return blocks_, blocks_names\n",
    "\n",
    "#=============================================================================\n",
    "# FPI only works if correlated features are permuted together.\n",
    "# Otherwise, correlated features permuted independently\n",
    "# will dilute the impact of that feature since the ML model\n",
    "# will still get some information from the unpermuted feature.\n",
    "#\n",
    "# --- Conventions ---\n",
    "# Within each group of features, the feature names are separated by commas.\n",
    "# The groups of features are separated by semi-colons.\n",
    "# Colons can be used for contiguous feature grouping, but this currently\n",
    "# ignored because need to come up with a reliable way to generalize processing\n",
    "# this case since it assumes the same feature names throughout.\n",
    "#\n",
    "# Inputs: Takes a list of feature names and a correlation heatmap between features\n",
    "# (One could just pass a .csv file and compute the correlation internally, but keep\n",
    "# separate for now to enable plotting and debugging.)\n",
    "def group_correlated_features(\n",
    "    feature_corr,\n",
    "    corr_cutoff=0.4,\n",
    "    merge_groups=False,\n",
    "    onehot_list=[],\n",
    "    verbose=False):\n",
    "    \n",
    "    # Take absolute value of the correlations.\n",
    "    abs_corr = np.abs(feature_corr)\n",
    "    \n",
    "    # First get rid of diagonal (but allowing for 1.0 correlations \n",
    "    # elsewhere, e.g. duplicate features)\n",
    "    feature_names_str = feature_corr.columns\n",
    "    for name in feature_names_str:\n",
    "        abs_corr.loc[name,name] = 0\n",
    "    \n",
    "    # Initialize tracker for highest correlation detected\n",
    "    current_highest_corr = 1.0\n",
    "    \n",
    "    # Initialize list of groups\n",
    "    # (Simple approach, ignoring one-hot features)\n",
    "    feature_groups_list = []\n",
    "    \n",
    "    # For one-hot features, we want to ensure all one-of-k streams\n",
    "    # are all permuted together.  This means that we pre-populate\n",
    "    # the feature_groups_list with the user specified one-hot \n",
    "    # features.  These features are identified by their prefix.\n",
    "    # If a prefix is in the list, a new group is created and\n",
    "    # all features in the list that match the prefix\n",
    "    # are automatically included in the group.  Later, if one\n",
    "    # (or more) of the one-hot streams in each one-hot feature\n",
    "    # correlates with another feature, those features can be\n",
    "    # merged, but all the streams from a one-hot feature will\n",
    "    # be carried as a block.\n",
    "    for prefix in onehot_list:\n",
    "        \n",
    "        if verbose:\n",
    "            print('Finding one-hot features based on given prefix: '+prefix)\n",
    "        # Create a new group for elements found with this prefix.\n",
    "        prefix_match = [s for s in feature_names_str if prefix in s]\n",
    "        \n",
    "        jj = 0\n",
    "        for feature in prefix_match:\n",
    "            if jj == 0:\n",
    "                if verbose:\n",
    "                    print('Creating one-hot group for feature: '+feature)\n",
    "                # Create the group with first feature\n",
    "                feature_groups_list.append(feature)\n",
    "                jj = 1\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Appending to one-hot group: '+feature)\n",
    "                feature_groups_list[-1] = feature_groups_list[-1]+','+feature\n",
    "    \n",
    "    # Initialize a counter to track number of times doing this\n",
    "    ii = 1\n",
    "    \n",
    "    while current_highest_corr >= corr_cutoff :\n",
    "        \n",
    "        # Scalar highest correlation in DF, could be duplicated\n",
    "        current_highest_corr = abs_corr.max().max()\n",
    "        \n",
    "        # Get locations of current_highest_corr, DF True at current_highest_corr\n",
    "        bool_current_highest_corr = abs_corr==current_highest_corr\n",
    "        \n",
    "        # Get DF of NaN except 1's at locations of current_highest_corr\n",
    "        loc_current_highest_corr = abs_corr[bool_current_highest_corr]/current_highest_corr\n",
    "        \n",
    "        # Count number of highest correlations in DF.  Divide by\n",
    "        # two because the corr DF is symmetric.\n",
    "        num_highest_corr = np.nansum(loc_current_highest_corr)/2\n",
    "                \n",
    "        if verbose:\n",
    "            print('Found highest correlation: '+str(current_highest_corr)+' with '+str(num_highest_corr)+' instances, iteration '+str(ii))\n",
    "        \n",
    "        # Find the features involved in this correlation\n",
    "        features_in_corr = []\n",
    "        index_current_highest_corr = np.where(bool_current_highest_corr)\n",
    "        for jj in index_current_highest_corr :\n",
    "            for kk in jj :\n",
    "                features_in_corr.append(abs_corr.index[kk])\n",
    "        \n",
    "        # Get a unique list of features in correlation\n",
    "        features_in_corr = list(set(features_in_corr))\n",
    "        features_in_corr_group_id = np.full(np.shape(features_in_corr),np.nan)\n",
    "        \n",
    "        # Check if any features_in_corr are already assigned in a group\n",
    "        for fid,feature in enumerate(features_in_corr) :\n",
    "            # Search for this feature in feature_groups_list\n",
    "            for gid,group in enumerate(feature_groups_list):\n",
    "                for feature_in_group in group.split(','):\n",
    "                    if (feature == feature_in_group):\n",
    "                        features_in_corr_group_id[fid] = gid\n",
    "        \n",
    "        if verbose:\n",
    "            print('Found correlation:-----------------------------------------------')\n",
    "            print(features_in_corr)\n",
    "            print('Starting gids:---------------------------------------------------')\n",
    "            print(features_in_corr_group_id)\n",
    "            print('Starting feature list:-------------------------------------------')\n",
    "            print(feature_groups_list)\n",
    "        \n",
    "        if np.nansum(np.isfinite(features_in_corr_group_id)) == 0:\n",
    "            if verbose:\n",
    "                print('No features already in group.')\n",
    "            # None of the features in this corr are in a grouping so\n",
    "            # make a new group with all features.\n",
    "            for fid,feature in enumerate(features_in_corr):\n",
    "                if fid == 0:\n",
    "                    # Create group for first feature\n",
    "                    if verbose:\n",
    "                        print('Creating new group----------------------------<===')\n",
    "                    feature_groups_list.append(feature)\n",
    "                else:\n",
    "                    # Add features to the group\n",
    "                    feature_groups_list[-1] = feature_groups_list[-1]+','+feature\n",
    "                    \n",
    "        elif np.nansum(np.isfinite(features_in_corr_group_id)) == 1:\n",
    "            if verbose:\n",
    "                print('Exactly one feature already in group.')\n",
    "            # Exactly one of the features is already in a group so\n",
    "            # Add the other features (with group id == NaN) to that group.\n",
    "            # Find the group id of the ONE feature already in a group:\n",
    "            gid = int(np.nansum(features_in_corr_group_id))\n",
    "            for fid,feature in enumerate(features_in_corr):\n",
    "                if np.isnan(features_in_corr_group_id[fid]):\n",
    "                    feature_groups_list[gid] = feature_groups_list[gid]+','+feature\n",
    "        else:\n",
    "            # There is some combination of at least two \n",
    "            # features in existing groups and some other\n",
    "            # feaures may or may not be in groups.  Some\n",
    "            # of the features could already be in the same\n",
    "            # group.  For cases when there there always exactly\n",
    "            # one correlation pair detected, then no new groups\n",
    "            # are created with merge_groups=False because the\n",
    "            # two correlated features already belong to different\n",
    "            # groups.\n",
    "            if np.all(features_in_corr_group_id == features_in_corr_group_id[0]):\n",
    "                # Special case when all features detected are \n",
    "                # already assigned to the same group.\n",
    "                if verbose:\n",
    "                    print('All features already in same group. Do nothing.')\n",
    "            elif merge_groups:\n",
    "                if verbose:\n",
    "                    print('Merge all groups/new features for this correlation.')\n",
    "                # Merge any groups associated with any feature detected here.\n",
    "                jj = 0\n",
    "                gid_merged = []\n",
    "                for fid,feature in enumerate(features_in_corr):\n",
    "                    gid = features_in_corr_group_id[fid]\n",
    "                    if verbose:\n",
    "                        print('For feature: '+feature+' with gid: '+str(gid))\n",
    "                    if (jj == 0):\n",
    "                        # Always create a new group for the supergroup we're\n",
    "                        # about to build.\n",
    "                        if verbose:\n",
    "                            print('Creating new group----------------------------<===')\n",
    "                        if np.isnan(gid):\n",
    "                            # The first feature does not have a group, so create\n",
    "                            # the new supergroup with just it.\n",
    "                            if verbose:\n",
    "                                print('New group is feature: '+feature)\n",
    "                            feature_groups_list.append(feature)\n",
    "                        else:\n",
    "                            # Convert gid to int now that we know that gid is not NaN\n",
    "                            gid = int(gid)\n",
    "                            \n",
    "                            # The first feature has a group, so create the new supergroup\n",
    "                            # as a duplicate of the first feature's group.\n",
    "                            if verbose:\n",
    "                                print('New group is duplicate group: '+feature_groups_list[gid])\n",
    "                            feature_groups_list.append(feature_groups_list[gid])\n",
    "                            \n",
    "                            # Keep track of which groups have already been merged.\n",
    "                            gid_merged.append(gid)\n",
    "                        jj = 1\n",
    "                    else:\n",
    "                        # The new supergroup exists, so append group/feature names\n",
    "                        # if it hasn't been appended before.\n",
    "                        if np.isnan(gid):\n",
    "                            # Feature with no group, append to supergroup\n",
    "                            if verbose:\n",
    "                                print('Feature with no group -> simple append')\n",
    "                                print('Appending feature: '+feature+' to group: '+feature_groups_list[-1])\n",
    "                            feature_groups_list[-1] = feature_groups_list[-1]+','+feature\n",
    "                        else:\n",
    "                            # Convert gid to int now that we know that gid is not NaN\n",
    "                            gid = int(gid)\n",
    "                            \n",
    "                            if gid in gid_merged:\n",
    "                                # Feature with a group but group has already been merged.\n",
    "                                if verbose:\n",
    "                                    print('Group already appended, skip this feature.')\n",
    "                            else:\n",
    "                                # Feature with a group, group has not already \n",
    "                                # been appended.  Append now.\n",
    "                                if verbose:\n",
    "                                    print('Group not already appended.')\n",
    "                                    print('Appending group: '+feature_groups_list[gid]+' to group: '+feature_groups_list[-1])\n",
    "                                feature_groups_list[-1] = feature_groups_list[-1]+','+feature_groups_list[gid]\n",
    "                                gid_merged.append(gid)\n",
    "                # Now that we are done creating the new supergroup, clean up\n",
    "                # by deleting the existing groups that have been merged into\n",
    "                # the supergroup.  Note that removal of merged groups MUST\n",
    "                # proceed in sorted, high GID to low GID because the length\n",
    "                # of the list is changed with pop operations, so the GIDs are\n",
    "                # reset.  Also add list(set()) inside to ensure that if there\n",
    "                # are duplicate groups being merged, there is only one group\n",
    "                # delete operation.\n",
    "                if verbose:\n",
    "                    print('Delete merged groups.')\n",
    "                    print(gid_merged)\n",
    "                    print(list(set(gid_merged)))\n",
    "                for gid in sorted(list(set(gid_merged)), reverse=True):\n",
    "                    if verbose:\n",
    "                        print('Deleting merged group '+feature_groups_list[gid]+'----------------------<===')\n",
    "                    feature_groups_list.pop(gid)\n",
    "                \n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Leave existing groups, create new group for extra features.')\n",
    "                # Leave any existing groups separate. Any features not \n",
    "                # associated with a group initiate their own group\n",
    "                jj = 0\n",
    "                for fid,feature in enumerate(features_in_corr):\n",
    "                    if np.isnan(features_in_corr_group_id[fid]):\n",
    "                        # Add this feature to new group\n",
    "                        if jj == 0:\n",
    "                            # First need to create the new group\n",
    "                            if verbose:\n",
    "                                print('Creating new group '+feature+'----------------------------<===')\n",
    "                            feature_groups_list.append(feature)\n",
    "                            jj = 1\n",
    "                        else:\n",
    "                            # Add features to the new group\n",
    "                            feature_groups_list[-1] = feature_groups_list[-1]+','+feature\n",
    "        \n",
    "        # Move to the next highest correlation\n",
    "        abs_corr[abs_corr==current_highest_corr] = 0\n",
    "        ii = ii + 1\n",
    "        \n",
    "    # Any remaining features are permuted independently and so\n",
    "    # by default do not need to be included in the group lists.\n",
    "    \n",
    "    # Concatenate feature_groups_list to feature_groups_str.\n",
    "    jj = 0\n",
    "    ff = 0\n",
    "    ff_list = []\n",
    "    for group in feature_groups_list:\n",
    "        if jj == 0:\n",
    "            feature_groups_str = group\n",
    "            jj = 1\n",
    "        else:\n",
    "            feature_groups_str = feature_groups_str+\";\"+group\n",
    "            \n",
    "        # Count features and check for duplicates\n",
    "        for feature in group.split(','):\n",
    "            ff = ff + 1\n",
    "            for feature_already_seen in ff_list:\n",
    "                if feature == feature_already_seen:\n",
    "                    print('WARNING: Duplicate feature detected: '+str(feature)+' '+str(ff))\n",
    "            ff_list.append(feature)\n",
    "            \n",
    "    # Print summary\n",
    "    print('Started with '+str(len(feature_names_str))+' features.')\n",
    "    print('Finishing with '+str(ff)+' features in '+str(len(feature_groups_list))+' groups.')\n",
    "    \n",
    "    return feature_groups_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute traditional correlations between all inputs and plot.\n",
    "# With lots of data, this is hard to interpret!\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "corr = all_df.corr()\n",
    "short_names = [name[:12] for name in corr.columns]\n",
    "sns.heatmap(ax=ax, data=np.abs(corr), xticklabels=short_names, yticklabels=short_names, cmap=sns.diverging_palette(220, 10, as_cmap=True,n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "group_correlated_features(\n",
    "    corr,\n",
    "    corr_cutoff=0.5,\n",
    "    merge_groups=True,\n",
    "    onehot_list=['General_Vegetation','River_Gradient','Sediment','Deposition','Hydrogeomorphology'],\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: What is the distribution of correlations?\n",
    "# Is there a particular correlation cutoff that is relevant for this data set?\n",
    "# In processing corr for plotting, first grab the lower triangle of the correlation\n",
    "# heat map since the heat map is symmetric.  Then, reshape it to a vector, and then take\n",
    "# the absolute value since we treat negative and positive correlations as the same.\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "n, bins, patches = ax.hist(np.reshape(np.tril(np.abs(corr)),-1), 20, density=False, facecolor='g', alpha=0.75, align='mid', histtype='stepfilled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Which features should be grouped together?\n",
    "# Data that are inherently linked (i.e. one-hot and categorical features)?\n",
    "# Expert knowledge?\n",
    "# Cluster remaining data based on correlation?\n",
    "corr_cutoff = 0.5\n",
    "hot_spots = corr[np.abs(corr) >= corr_cutoff]\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(ax=ax, data=np.abs(hot_spots), xticklabels=short_names, yticklabels=short_names, cmap=sns.diverging_palette(220, 10, as_cmap=True,n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: FPI based on the grouped features\n",
    "\n",
    "#==========================================================\n",
    "# General settings here\n",
    "#==========================================================\n",
    "\n",
    "job_list = [jobid]\n",
    "permute_str = group_correlated_features(\n",
    "    corr,\n",
    "    corr_cutoff=0.5,\n",
    "    merge_groups=True,\n",
    "    onehot_list=['General_Vegetation','River_Gradient','Sediment','Deposition','Hydrogeomorphology'],\n",
    "    verbose=False)\n",
    "\n",
    "#==========================================================\n",
    "# Run FPI for stacked model and each individual submodel\n",
    "#==========================================================\n",
    "model_fpi_results = list()\n",
    "sl_fpi_results = list()\n",
    "\n",
    "i = 0\n",
    "for job_id in job_list:\n",
    "    \n",
    "    print('Loading model for job: '+str(job_id))\n",
    "    sl = pickle.load(open('/pw/jobs/'+str(job_id)+'/model_dir/SuperLearners.pkl', 'rb'))\n",
    "\n",
    "    #----------------------------------------------------\n",
    "    # FPI for stacked model\n",
    "    #----------------------------------------------------\n",
    "    model_object = sl[target_name]\n",
    "    \n",
    "    print('FPI on stacked ensemble...')\n",
    "    result = permute_importance(permute_str, \n",
    "            model_object,\n",
    "            all_df, \n",
    "            target_all_df,\n",
    "            mean_squared_error)\n",
    "    # Convert back to dataframe, consider using MultiIndex\n",
    "    # functionality instead of the clunky filter below.\n",
    "    result_df = pd.DataFrame(result,\n",
    "                             columns=['Feature',\n",
    "                                      'Avg_Ratio'+'stack'+str(job_id), \n",
    "                                      'Std_Ratio'+'stack'+str(job_id)]).set_index('Feature')\n",
    "    \n",
    "    sl_fpi_results.append(result_df)\n",
    "    \n",
    "    #----------------------------------------------------\n",
    "    # FPI for each submodel individually\n",
    "    #----------------------------------------------------\n",
    "    for model_name in sl_models:\n",
    "        model_object = sl[target_name].named_estimators_[model_name]\n",
    "        \n",
    "        print('FPI on ML model: '+model_name+'...')\n",
    "        result = permute_importance(permute_str, \n",
    "            model_object,\n",
    "            all_df, \n",
    "            target_all_df,\n",
    "            mean_squared_error)\n",
    "        result_df = pd.DataFrame(result,\n",
    "            columns=['Feature',\n",
    "            'Avg_Ratio'+model_name+str(job_id), \n",
    "            'Std_Ratio'+model_name+str(job_id)]).set_index('Feature')\n",
    "        \n",
    "        model_fpi_results.append(result_df)\n",
    "\n",
    "# Merge all dataframes into a single frame with\n",
    "# features as the index.\n",
    "sl_fpi_results_df = pd.concat(sl_fpi_results,axis=1)\n",
    "model_fpi_results_df = pd.concat(model_fpi_results,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked model results\n",
    "print(sl_fpi_results_df.sort_values(by='Avg_Ratiostack'+str(jobid),axis=0,ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All other models\n",
    "for model in sl_models:\n",
    "    print('--------'+model+'---------')\n",
    "    print(model_fpi_results_df['Avg_Ratio'+model+str(jobid)].sort_values(axis=0,ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view a particular model's list,\n",
    "# Choose from ['nusvr', 'mlp', 'ridge', 'xgb']\n",
    "# Choose from ['Avg_Ratio', 'Std_Ratio']\n",
    "pd.DataFrame(\n",
    "    model_fpi_results_df.filter(\n",
    "        like='nusvr',axis=1).filter(\n",
    "        like='Avg_Ratio',axis=1).mean(axis=1)).sort_values(\n",
    "            by=0,\n",
    "            axis=0,\n",
    "            ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
