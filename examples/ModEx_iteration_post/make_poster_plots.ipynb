{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocess SuperLearner results\n",
    "\n",
    "We have run several ModEx iterations and now we need to visualize the results! This notebook assumes that all the ModEx iterations have been staged back to a single location and are present at the same time (rather than using `git checkout <sha1>` and [browsing the repository in detached head status](https://stackoverflow.com/questions/7539130/go-to-particular-revision)).\n",
    "\n",
    "There are two main tasks done here:\n",
    "1. Consolidating the `hold-out-metrics` for each machine learning model created during the workflow so we can plot the evolution of the \"score\" of the ML models with each ModEx iteration. The plot is also done here.\n",
    "2. Consolidating the data in `sl_pca.csv` (i.e. the `combined.metric` used to evaluate the relative \"importance\" of all potential sites) so we can visualize the progression of the `combined.metric` on a site-by-site basis. Only the consolidation is done here; the final map plotting is gone in [GMT](https://www.generic-mapping-tools.org/) in `./make_poster_maps.sh`, so not in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate `hold-out-metrics`\n",
    "\n",
    "The `hold-out-metric` is the model score when the model makes predictions with test data (data **not** used to train the model); it is therefore an estimate of the performance of the model when it is used to make predictions with new data the model has not yet seen.  We need to repeat the training/evaluation process so that different subsets of the input data are used to train and test the model. Here, we need to collect the `hold-out-metric` from each instance of the SuperLearner and then we will find the average and standard devitation associated with each ModEx iteration.\n",
    "\n",
    "The first cell below consolidates the data.  The second cell below plots from the consolidated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the basedir for the ModEx iteration jobs\n",
    "basedir = \"/pw/jobs/\"\n",
    "infix = \"/ml_models/sl_\"\n",
    "postfix = \"/hold-out-metrics.json\"\n",
    "num_models = 10\n",
    "modex_branch_dict = {\n",
    "    \"modex_random_add\": [57042, 57043, 57044, 57045, 57046],\n",
    "    \"modex_ordered_add\": [57048, 57049, 57050, 57051, 57052]\n",
    "}\n",
    "\n",
    "# Loop over each branch\n",
    "for branch in list(modex_branch_dict.keys()):\n",
    "    print(\"Working on branch \"+branch)\n",
    "    \n",
    "    # Open output file\n",
    "    with open(branch+\".xyz\", \"a\") as out_file:\n",
    "        # Write header line\n",
    "        out_file.write(\"iter avg std\\n\")\n",
    "    \n",
    "        # Loop over each ModEx random add iteration\n",
    "        for count, job in enumerate(list(modex_branch_dict[branch])):\n",
    "            print(\"Working on job \"+str(job))\n",
    "            \n",
    "            # Allocate empty list\n",
    "            hold_out = []\n",
    "\n",
    "            for model in range(0, num_models, 1):\n",
    "                with open(basedir+str(job)+infix+str(model)+postfix, 'r') as in_file:\n",
    "                    # Load hold out metric from JSON file\n",
    "                    raw_hold_out = list(json.load(in_file).values())[0]\n",
    "            \n",
    "                    # If this value is less than zero, make it zero\n",
    "                    # (Applied to models that do worse than a\n",
    "                    # \"persistance forecast\")\n",
    "                    if raw_hold_out < 0:\n",
    "                        hold_out.append(0)\n",
    "                    else:\n",
    "                        hold_out.append(raw_hold_out)\n",
    "            \n",
    "                    # Helpful for debugging\n",
    "                    #print(str(count)+\"   \"+str(raw_hold_out)+\"--->\"+str(hold_out))\n",
    "                \n",
    "            # Write output\n",
    "            out_file.write(str(count)+\" \"+str(np.average(hold_out))+\" \"+str(np.std(hold_out))+\"\\n\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data saved above\n",
    "modex_random_add = pd.read_csv('modex_random_add.xyz', sep=\" \", )\n",
    "modex_ordered_add = pd.read_csv('modex_ordered_add.xyz', sep=\" \", )\n",
    "\n",
    "# Plot data\n",
    "fig, ax = plt.subplots()\n",
    "plt.errorbar(modex_random_add['iter'],modex_random_add['avg'],yerr=modex_random_add['std'],xerr=None,fmt='k-',capsize=10)\n",
    "plt.errorbar(modex_ordered_add['iter'],modex_ordered_add['avg'],yerr=modex_ordered_add['std'],xerr=None,fmt='r-',capsize=10)\n",
    "ax.legend([\"Random add\",\"Ordered add\"])\n",
    "ax.grid()\n",
    "plt.savefig('model_score_vs_modex_iter.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate `sl_pca.csv` results\n",
    "\n",
    "The last step in the [ModEx SuperLearner workflow](https://github.com/parallelworks/sl_core/blob/main/train_predict_eval.sh) is to use principal component analysis (PCA) along with error estimates for the predictions to create a `combined.metric` that predicts the \"potential importance\" of a possible new data point. If a data point is relatively different from other data points (as quantified by its distance from the PCA centroid) and it has a relatively large predicted error, we hypothesize that this data point would add valuable information to the machine learning.  If these data points with relatively high `combined.metric` are prioritized as data are added to the machine learning, then we expect models trained with the priority (i.e. \"important\") data points will perform better than models trained with less of these priority data points.\n",
    "\n",
    "Plotting the `hold-out-metrics` (above) is one \"global\" way to assess the evolution of the models. However, it may also be interesting to look for patterns in the progression/stability/volitility of the `combined.metric` on a site-by-site basis.  Are there sites that are consistently, over the course of the ModEx iterations, ranked as \"high (low) priority\"? Are there sites whose priority grows (decreases) consistently over the course of the ModEx iterations? We plan to evaluate both questions in light of any geospatial patterns in the distribution of these sites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the basedir for the ModEx iteration jobs\n",
    "basedir = \"/pw/jobs/\"\n",
    "infix = \"/ml_models/sl_\"\n",
    "postfix = \"/sl_pca.csv\"\n",
    "num_models = 10\n",
    "modex_branch_dict = {\n",
    "    \"modex_random_add\": [57042, 57043, 57044, 57045, 57046],\n",
    "    \"modex_ordered_add\": [57048, 57049, 57050, 57051, 57052]\n",
    "}\n",
    "\n",
    "# Loop over each branch\n",
    "for branch in list(modex_branch_dict.keys()):\n",
    "    print(\"Working on branch \"+branch)\n",
    "    \n",
    "    # Allocate a blank dataframe to store combined \n",
    "    # metric for each site for each iteration\n",
    "    all_iter_df = pd.DataFrame({\"ID\": []})\n",
    "    \n",
    "    # Loop over each ModEx random add iteration\n",
    "    for count, job in enumerate(list(modex_branch_dict[branch])):\n",
    "        print(\"Working on job \"+str(job))\n",
    "\n",
    "        for model in range(0, num_models, 1):\n",
    "            one_iter_df = pd.read_csv(basedir+str(job)+infix+str(model)+postfix)\n",
    "            if len(all_iter_df) == 0:\n",
    "                # First time, copy IDs into dataframe\n",
    "                all_iter_df['ID'] = one_iter_df['GL_id']\n",
    "                all_iter_df['lon'] = one_iter_df['lon']\n",
    "                all_iter_df['lat'] = one_iter_df['lat']\n",
    "                \n",
    "                # Grab the combined.metric and append to the file\n",
    "                all_iter_df[str(count)+\"_\"+str(model)] = one_iter_df['combined.metric']\n",
    "            else:\n",
    "                # Check that the IDs are the same\n",
    "                if np.sum(all_iter_df['ID'] - one_iter_df['GL_id']) > 0:\n",
    "                    print('WARNING: Site ID mismatch!')\n",
    "                    \n",
    "                # Grab the combined.metric and append to the file\n",
    "                all_iter_df[str(count)+\"_\"+str(model)] = one_iter_df['combined.metric']\n",
    "    #================================================================            \n",
    "    # Done with gathering data from each iteration on this branch.\n",
    "    # Now, compute statistics associated with each site over all \n",
    "    # iterations. TODO: Generalize code for more ModEx iterations,\n",
    "    # currently hard coded for 5.\n",
    "    \n",
    "    # Sum of all x values - sum of all interation counters, e.g.\n",
    "    # iteration 1, 2, 3, 4, 5 for all models. HARD CODED\n",
    "    all_iter_df['Sx'] = all_iter_df['ID']*0 + num_models*(1+2+3+4+5)\n",
    "    all_iter_df['Sx2'] = all_iter_df['ID']*0 + num_models*(1+4+9+16+25)\n",
    "    \n",
    "    # Sum of all y values - sum of all combined metrics for each site\n",
    "    # over all models and all iterations\n",
    "    mask = all_iter_df.columns.str.contains('_')\n",
    "    all_iter_df['Sy'] = all_iter_df.loc[:,mask].sum(axis=1)\n",
    "\n",
    "    # Need to compute mask again b/c just appended a column\n",
    "    mask = all_iter_df.columns.str.contains('_')\n",
    "    df = all_iter_df.loc[:,mask]*all_iter_df.loc[:,mask]\n",
    "    all_iter_df['Sy2'] = df.sum(axis=1)\n",
    "    \n",
    "    # Sum of product of all x and y values HARD CODED\n",
    "    mask1 = all_iter_df.columns.str.contains('0_.')\n",
    "    mask2 = all_iter_df.columns.str.contains('1_.')\n",
    "    mask3 = all_iter_df.columns.str.contains('2_.')\n",
    "    mask4 = all_iter_df.columns.str.contains('3_.')\n",
    "    mask5 = all_iter_df.columns.str.contains('4_.')\n",
    "    \n",
    "    df1 = all_iter_df.loc[:,mask1]*1.0\n",
    "    df2 = all_iter_df.loc[:,mask2]*2.0\n",
    "    df3 = all_iter_df.loc[:,mask3]*3.0\n",
    "    df4 = all_iter_df.loc[:,mask4]*4.0\n",
    "    df5 = all_iter_df.loc[:,mask5]*5.0\n",
    "    \n",
    "    all_iter_df['Sxy'] = df1.sum(axis=1) + df2.sum(axis=1) + df3.sum(axis=1) + df4.sum(axis=1) + df5.sum(axis=1)\n",
    "    \n",
    "    # Intermediate values for linear regression\n",
    "    all_iter_df['SSxx'] = all_iter_df['Sx2'] - all_iter_df['Sx']*all_iter_df['Sx']/(num_models*len(list(modex_branch_dict[branch])))\n",
    "    all_iter_df['SSyy'] = all_iter_df['Sy2'] - all_iter_df['Sy']*all_iter_df['Sy']/(num_models*len(list(modex_branch_dict[branch])))\n",
    "    all_iter_df['SSxy'] = all_iter_df['Sxy'] - all_iter_df['Sx']*all_iter_df['Sy']/(num_models*len(list(modex_branch_dict[branch])))\n",
    "\n",
    "    # Mean and std over all iterations over all models\n",
    "    # Need to compute mask each time b/c appending cols\n",
    "    mask = all_iter_df.columns.str.contains('_')\n",
    "    all_iter_df['avg'] = all_iter_df.loc[:,mask].mean(axis=1)\n",
    "    \n",
    "    mask = all_iter_df.columns.str.contains('_')\n",
    "    all_iter_df['std'] = all_iter_df.loc[:,mask].std(axis=1)\n",
    "    \n",
    "    # Results from linear regression\n",
    "    all_iter_df['slope'] = all_iter_df['SSxy']/all_iter_df['SSxx']\n",
    "    all_iter_df['r'] = all_iter_df['SSxy']/np.sqrt(all_iter_df['SSxx']*all_iter_df['SSyy'])\n",
    "    all_iter_df['r2'] = all_iter_df['r']*all_iter_df['r']\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working here to cross check one Sxy value.\n",
    "# Values are off by less than 10%?!\n",
    "ii = 10441\n",
    "print(np.sum(all_iter_df.iloc[ii,3:13]*1.0) + np.sum(all_iter_df.iloc[ii,14:23]*2.0) + np.sum(all_iter_df.iloc[ii,24:33]*3.0) + np.sum(all_iter_df.iloc[ii,34:43]*4.0) + np.sum(all_iter_df.iloc[ii,44:53]*5.0))\n",
    "#print(np.sum(all_iter_df.iloc[0,3:13]*1.0))\n",
    "#print(np.sum(all_iter_df.iloc[0,14:23]*2.0))\n",
    "#print(np.sum(all_iter_df.iloc[0,24:33]*3.0))\n",
    "#print(np.sum(all_iter_df.iloc[0,34:43]*4.0))\n",
    "#print(np.sum(all_iter_df.iloc[0,44:53]*5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_iter_df['Sxy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print output file\n",
    "all_iter_df.to_csv('sl_pca_consolidated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
